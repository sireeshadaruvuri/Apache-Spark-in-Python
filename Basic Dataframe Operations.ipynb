{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc0|          29|\n",
      "|03/04/2019 19:53:06|       loc0|          27|\n",
      "|03/04/2019 19:58:06|       loc0|          28|\n",
      "|03/04/2019 20:03:06|       loc0|          30|\n",
      "|03/04/2019 20:08:06|       loc0|          27|\n",
      "|03/04/2019 20:13:06|       loc0|          27|\n",
      "|03/04/2019 20:18:06|       loc0|          27|\n",
      "|03/04/2019 20:23:06|       loc0|          29|\n",
      "|03/04/2019 20:28:06|       loc0|          32|\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 20:38:06|       loc0|          32|\n",
      "|03/04/2019 20:43:06|       loc0|          28|\n",
      "|03/04/2019 20:48:06|       loc0|          28|\n",
      "|03/04/2019 20:53:06|       loc0|          32|\n",
      "|03/04/2019 20:58:06|       loc0|          34|\n",
      "|03/04/2019 21:03:06|       loc0|          33|\n",
      "|03/04/2019 21:08:06|       loc0|          27|\n",
      "|03/04/2019 21:13:06|       loc0|          28|\n",
      "|03/04/2019 21:18:06|       loc0|          33|\n",
      "|03/04/2019 21:23:06|       loc0|          28|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Loading csv files into dataframes\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate() \n",
    "spark\n",
    "data_path = '/Siri F1 assignments/apache spark/LinkedIn learning/course 2/Data'\n",
    "file_path = data_path +'/location_temp.csv'\n",
    "df1 = spark.read.format('csv').option(\"header\",\"true\").load(file_path)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['event_date', 'location_id', 'temp_celcius']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple random sampling in pyspark with example\n",
    "In Simple random sampling every individuals are randomly obtained and so the individuals are equally likely to be chosen.\n",
    "Simple Random sampling in pyspark is achieved by using sample() Function. \n",
    "\n",
    "Syntax:\n",
    "sample(False, fraction, seed=None)\n",
    "\n",
    "Returns a sampled subset of Dataframe without replacement.\n",
    "\n",
    "Note: fraction is not guaranteed to provide exactly the fraction specified in Dataframe\n",
    "\n",
    "Simple random sampling with replacement\n",
    "\n",
    "Syntax:\n",
    "\n",
    " sample(True, fraction, seed=None)\n",
    "Returns a sampled subset of Dataframe with replacement.\n",
    "\n",
    "Stratified sampling in pyspark\n",
    "In Stratified sampling every member of the population is grouped into homogeneous subgroups called strata and representative of each group (strata) is chosen.\n",
    "\n",
    "Syntax:\n",
    "\n",
    "sampleBy(col, fractions, seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 20:33:06|       loc0|          35|\n",
      "|03/04/2019 21:33:06|       loc0|          27|\n",
      "|03/04/2019 21:48:06|       loc0|          27|\n",
      "|03/04/2019 22:03:06|       loc0|          29|\n",
      "|03/04/2019 22:13:06|       loc0|          29|\n",
      "|03/04/2019 22:18:06|       loc0|          29|\n",
      "|03/04/2019 22:38:06|       loc0|          27|\n",
      "|03/05/2019 00:23:06|       loc0|          30|\n",
      "|03/05/2019 01:08:06|       loc0|          27|\n",
      "|03/05/2019 02:38:06|       loc0|          29|\n",
      "|03/05/2019 02:53:06|       loc0|          30|\n",
      "|03/05/2019 03:23:06|       loc0|          31|\n",
      "|03/05/2019 03:28:06|       loc0|          29|\n",
      "|03/05/2019 03:38:06|       loc0|          27|\n",
      "|03/05/2019 04:18:06|       loc0|          28|\n",
      "|03/05/2019 06:03:06|       loc0|          27|\n",
      "|03/05/2019 06:53:06|       loc0|          28|\n",
      "|03/05/2019 07:23:06|       loc0|          29|\n",
      "|03/05/2019 07:33:06|       loc0|          27|\n",
      "|03/05/2019 07:53:06|       loc0|          27|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_sample = df1.sample(False,fraction=0.1)\n",
    "df1_sample.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sort() or orderBy() functions can be used in PySpark DataFrame to sort DataFrame by ascending or descending order based on single or multiple columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------+------------+\n",
      "|         event_date|location_id|temp_celcius|\n",
      "+-------------------+-----------+------------+\n",
      "|03/04/2019 19:48:06|       loc2|          28|\n",
      "|03/04/2019 19:48:07|       loc5|          25|\n",
      "|03/04/2019 19:48:09|      loc18|          35|\n",
      "|03/04/2019 19:48:12|      loc32|          29|\n",
      "|03/04/2019 19:48:15|      loc46|          28|\n",
      "|03/04/2019 19:48:18|      loc65|          32|\n",
      "|03/04/2019 19:48:19|      loc68|          27|\n",
      "|03/04/2019 19:48:26|     loc108|          39|\n",
      "|03/04/2019 19:48:28|     loc117|          30|\n",
      "|03/04/2019 19:48:28|     loc115|          23|\n",
      "|03/04/2019 19:48:29|     loc121|          23|\n",
      "|03/04/2019 19:48:29|     loc123|          21|\n",
      "|03/04/2019 19:48:32|     loc140|          37|\n",
      "|03/04/2019 19:48:32|     loc136|          32|\n",
      "|03/04/2019 19:48:33|     loc141|          25|\n",
      "|03/04/2019 19:48:34|     loc150|          30|\n",
      "|03/04/2019 19:48:34|     loc148|          29|\n",
      "|03/04/2019 19:48:36|     loc158|          25|\n",
      "|03/04/2019 19:48:37|     loc167|          32|\n",
      "|03/04/2019 19:48:39|     loc176|          24|\n",
      "+-------------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1_sort = df1_sample.sort('event_date')\n",
    "df1_sort.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
